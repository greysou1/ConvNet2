{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from cnn import ConvNet \n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np     \n",
    "import matplotlib.pyplot as plt\n",
    "from cProfile import label\n",
    "import json\n",
    "import sys\n",
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device selected:  cpu\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set proper device based on cuda availability \n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Torch device selected: \", device)\n",
    "\n",
    "# Create transformations to apply to each data sample \n",
    "# Can specify variations such as image flip, color flip, random crop, ...\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.0,), (1.0,))\n",
    "        ])\n",
    "\n",
    "# Load datasets for training and testing\n",
    "# Inbuilt datasets available in torchvision (check documentation online)\n",
    "dataset1 = datasets.CIFAR10('./data/', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.CIFAR10('./data/', train=False,\n",
    "                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=10, num_workers=4):\n",
    "    train_loader = DataLoader(dataset1, batch_size = batch_size, \n",
    "                            shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(dataset2, batch_size = batch_size, \n",
    "                                shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def plot(num_epochs, losses, accuracies, best_accuracy, save=0, mode=1):\n",
    "    x = range(1, num_epochs+1)\n",
    "\n",
    "    train_accuracies, test_accuracies = accuracies[0], accuracies[1]\n",
    "    train_losses, test_losses = losses[0], losses[1]\n",
    "    ## accuracies plot\n",
    "    plt.title(f'Model {mode} Accuracies')\n",
    "    plt.plot(x, train_accuracies, 'r')\n",
    "    plt.plot(x, test_accuracies, 'b')\n",
    "    plt.legend(['Train Accuracy', 'Test Accuracy'])\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f'plots/model_{mode}_accuracies.jpg')\n",
    "        plt.show()\n",
    "\n",
    "    ## losses plot\n",
    "    plt.title(f'Model {mode} Losses')\n",
    "    plt.plot(x, train_losses, 'r')\n",
    "    plt.plot(x, test_losses, 'b')\n",
    "    plt.legend(['Train Losses', 'Test Losses'])\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(f'plots/model_{mode}_losses.jpg')\n",
    "        plt.show()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch, batch_size, num_epochs):\n",
    "    '''\n",
    "    Trains the model for an epoch and optimizes it.\n",
    "    model: The model to train. Should already be in correct device.\n",
    "    device: 'cuda' or 'cpu'.\n",
    "    train_loader: dataloader for training samples.\n",
    "    optimizer: optimizer to use for model parameter updates.\n",
    "    criterion: used to compute loss for prediction and target \n",
    "    epoch: Current epoch to train for.\n",
    "    batch_size: Batch size to be used.\n",
    "    '''\n",
    "    \n",
    "    # Set model to train mode before each epoch\n",
    "    model.train()\n",
    "    \n",
    "    # Empty list to store losses \n",
    "    losses = []\n",
    "    correct = 0\n",
    "    # Iterate over entire training samples (1 epoch)\n",
    "    for batch_idx, batch_sample in enumerate(train_loader):\n",
    "        # if batch_idx == 3:\n",
    "        #     break\n",
    "        data, target = batch_sample\n",
    "        # print(f'{data.shape = }')\n",
    "        \n",
    "        # Push data/label to correct device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Do forward pass for current set of data\n",
    "        output = model(data)\n",
    "        # print(output.detach().numpy())\n",
    "        # Compute loss based on criterion\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Computes gradient based on final loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Store loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Optimize model parameters based on learning rate and gradient \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get predicted index by selecting maximum log-probability\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        _, predictions = output.max(1)\n",
    "        correct += (predictions == target).sum()\n",
    "        print(f'Training epoch: ({epoch}/{num_epochs}) batch: ({batch_idx+1}/{len(train_loader)})', end='\\r') #. Acc: {correct}/{(batch_idx+1) * batch_size}, {100. * correct / ((batch_idx+1) * batch_size)}', end='\\r')\n",
    "        \n",
    "    train_loss = float(np.mean(losses))\n",
    "    train_acc = 100. * (correct / ((batch_idx+1) * batch_size))\n",
    "    print('\\nTrain set ({}/{}): Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch, num_epochs,\n",
    "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size, train_acc))\n",
    "    return train_loss, train_acc\n",
    "    \n",
    "def test(model, device, test_loader, criterion, epoch, num_epochs, batch_size):\n",
    "    '''\n",
    "    Tests the model.\n",
    "    model: The model to train. Should already be in correct device.\n",
    "    device: 'cuda' or 'cpu'.\n",
    "    test_loader: dataloader for test samples.\n",
    "    '''\n",
    "    \n",
    "    # Set model to eval mode to notify all layers.\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct = 0\n",
    "    \n",
    "    # Set torch.no_grad() to disable gradient computation and backpropagation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample in enumerate(test_loader):\n",
    "            data, target = sample\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Predict for data by doing forward pass\n",
    "            output = model(data)\n",
    "        \n",
    "            # Compute loss based on same criterion as training \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Append loss to overall test loss\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Get predicted index by selecting maximum log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            _, predictions = output.max(1)\n",
    "            correct += (predictions == target).sum()\n",
    "            print(f'Testing epoch: ({epoch}/{num_epochs}) batch: ({batch_idx+1}/{len(test_loader)})', end='\\r')\n",
    "\n",
    "    test_loss = float(np.mean(losses))\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set ({}/{}): Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch, num_epochs,\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "    \n",
    "    return test_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(mode=1, learning_rate=0.01, batch_size=10, num_epochs=60):\n",
    "    image_size = 32*32*3\n",
    "    num_classes = 10\n",
    "\n",
    "    # Initialize the model and send to device \n",
    "    model = ConvNet(mode, image_size, num_classes).to(device)\n",
    "    # Define loss function.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define optimizer function.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Define data loaders\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    # Run training for n_epochs specified in config \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader,\n",
    "                                            optimizer, criterion, epoch, batch_size, num_epochs)\n",
    "        test_loss, test_accuracy = test(model, device, test_loader, criterion, epoch, num_epochs, batch_size)\n",
    "        \n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        # print(train_accuracy.cpu().numpy())\n",
    "        train_accuracies.append(train_accuracy.cpu().numpy())\n",
    "        test_accuracies.append(test_accuracy.cpu().numpy())\n",
    "\n",
    "    accuracies = [train_accuracies, test_accuracies]\n",
    "    losses = [train_losses, test_losses]\n",
    "    plot(num_epochs, losses, accuracies, best_accuracy, save=1, mode=mode)\n",
    "\n",
    "    print(\"Accuracy: {:2.2f}%\".format(best_accuracy))\n",
    "\n",
    "    print(\"Training and evaluation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model 1 ==================\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "print('\\n\\n'+('='*32)+' Training model 1 '+('='*32))\n",
    "print('CNN architecture which has more than 2 conv layers (3) and more than 1 fully connected layers (2)')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "run_model(mode=1, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)    \n",
    "# print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Training model 2 ================================\n",
      "Increase the number of conv layers (5) in the above network and train again.\n",
      "\n",
      "learning_rate = 0.1\n",
      "batch_size = 64\n",
      "num_epochs = 30\n",
      "\n",
      "Training epoch: (1/30) batch: (782/782)\n",
      "Train set (1/30): Average loss: 2.3613, Accuracy: 4987/50048 (10%)\n",
      "\n",
      "Testing epoch: (1/30) batch: (157/157)\n",
      "Test set (1/30): Average loss: 2.3613, Accuracy: 1000/10000 (10%)\n",
      "\n",
      "Training epoch: (2/30) batch: (187/782)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/ck6qtb554x7fcmpxs24wxn1w0000gn/T/ipykernel_71736/980734462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d_/ck6qtb554x7fcmpxs24wxn1w0000gn/T/ipykernel_71736/2934387617.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(mode, learning_rate, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Run training for n_epochs specified in config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d_/ck6qtb554x7fcmpxs24wxn1w0000gn/T/ipykernel_71736/2697757023.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Computes gradient based on final loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Store loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================== Model 2 ==================\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "print(('='*32)+' Training model 2 '+('='*32))\n",
    "print('Increase the number of conv layers (5) in the above network and train again.')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "\n",
    "run_model(mode=2, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
